model_params:

  # Training 
  training_steps: 2000000      # The number of training steps to perform

  # Save params
  save_freq: 5000
  save_prefix: ppo_model
  trained_model_name: trained_model
  save_replay_buffer: False # PPO does not support saving replay buffer

  # Load model params
  load_model: False
  model_name: trained_model_01_10_2021_15_30_30

  # Logging parameters
  log_folder: PPO_5
  log_interval: 1 # The number of episodes between logs
  reset_num_timesteps: False # If true, will reset the number of timesteps to 0 every training 

  # Use custom policy - Only MlpPolicy is supported (Only used when new model is created)
  use_custom_policy: True
  policy_params:
    net_arch: [400, 300] # List of hidden layer sizes
    activation_fn: relu  # relu, tanh, elu or selu
    features_extractor_class: FlattenExtractor # FlattenExtractor, BaseFeaturesExtractor or CombinedExtractor
    optimizer_class: Adam # Adam, Adadelta, Adagrad, RMSprop or SGD

  # Use SDE
  use_sde: False
  sde_params:
    sde_sample_freq: -1

  # PPO parameters
  ppo_params:
    # learning_rate: 0.0003
    learning_rate: 0.0005
    # n_steps: 512    # The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)
    n_steps: 128    # The number of steps to run for each environment per update (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)
    batch_size: 128 # Minibatch size 2/26 64 -> 128
    # n_epochs: 100     # Number of epoch when optimizing the surrogate loss
    n_epochs: 50     # Number of epoch when optimizing the surrogate loss
    gamma: 0.99
    gae_lambda: 0.95
    # clip_range: 0.2
    clip_range: 0.15 # 2/26 0.2 -> 0.15
    # ent_coef: 0.0
    ent_coef: 0.1 # 2/26 0.0 -> 0.5 -> 0.3 -> 0.1
    vf_coef: 0.5
    max_grad_norm: 0.5